---
layout: post
title: "Cinema and Clouds"
subtitle: "Text Analysis Code-Through Tutorial"
image: /img/globalpop.png
---

This blog post will provide a step-by-step code-through guide on how to examine textual data and create word clouds in R!

# Introduction

Although data science projects often employ large amounts of numeric data, some projects examine patterns within text and require a different set of tools. In this tutorial, we are going to explore several packages in R that enable researchers to analyze qualitative data sets and discover really cool patterns. We are also going to create a couple of word clouds based on various movie genres.

# Library of Packages

Before we begin analyzing the data or creating a word cloud, we must first load a few packages into our 'library'. Specifically we need the following packages:

```{r}
library(pander)
library(dplyr)
library( quanteda )

```


# Explore Data

```{r}
df <- read.csv("https://query.data.world/s/rr46ndg7fyne54q7oonmvzxbaxg3zn", header=TRUE, stringsAsFactors=FALSE);
```

```{r}
colnames(df)<- tolower(colnames(df))
```

```{r}
dat <- df[c("title", "genre", "plot" )]
```

```{r}
head( dat ) %>% pander()
```

![](/charts/Chart1.png)

```{r}
grep( pattern="romance", x=dat$genre, value=TRUE, ignore.case=TRUE ) %>% head() %>% pander()
```

```{r}
romance <- grepl( "romance", dat$genre, ignore.case = T ) 
sum(romance)
```

```{r}
dat.romance <- dat[ romance, c("title", "genre", "plot") ]
dat.romance %>% head(10) %>% pander()
```

```{r}
corp.romance <- corpus( dat.romance, docid_field="title", text_field="plot" )
corp.romance
```

```{r}
corp.romance[1:5] 
```

```{r}
# summarize corpus
summary(corp.romance)[1:10,] 
```


```{r}
# pre-processing steps:

# remove mission statements that are less than 1 sentence long
corp.romance <- corpus_trim( corp.romance, what="sentences", min_ntoken=1 )
corp.romance

# remove punctuation 
tokens.romance <- tokens( corp.romance, what="word", remove_punct=TRUE )
head( tokens.romance )

# convert to lower case
tokens.romance <- tokens_tolower( tokens.romance, keep_acronyms=TRUE )
head( tokens.romance )
```


```{r}
tokens.romance <- tokens_remove( tokens.romance, c( stopwords("english"), "nbsp" ), padding=F )
head(tokens.romance)
```

```{r}
# stem the words in the token list: 
tokens.romance <- tokens_wordstem( tokens.romance )
head( tokens.romance )
```


```{r}
# find frequently co-occuring words (typically compound words)
ngram.romance <- tokens_ngrams( tokens.romance, n=2 ) %>% dfm()
ngram.romance %>% textstat_frequency( n=10 )
```

```{r}
ngram.romance3 <- tokens_ngrams( tokens.romance, n=3 ) %>% dfm()
ngram.romance3 %>% textstat_frequency( n=10 )
```

```{r}
tokens.romance %>% dfm( stem=T ) %>% topfeatures( )
```

### Word


---

<div class="tinytext" markdown="1">
 <p markdown="1"> Clip art credit: Gerd Altmann, [Pixabay](https://pixabay.com/illustrations/person-silhouettes-human-2829500/) </p>
</div>

<style>

.tinytext p{
font-size: xx-small
}

</style>
